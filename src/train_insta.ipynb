{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff0b58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58fcd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(filename='segnet.log', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d55e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters - according to paper\n",
    "NUM_EPOCHS = 1000\n",
    "BATCH_SIZE = 2\n",
    "LEARNING_RATE = 0.001\n",
    "IMG_SIZE = (512, 256)\n",
    "NUM_WORKERS =  0#16\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(DEVICE)\n",
    "logger.info(f'Device: {DEVICE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595da8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init: Datasets and Dataloader\n",
    "import os\n",
    "from SegCropNet.dataloader.data_loaders import TusimpleSet\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_path = os.path.join(os.path.abspath(''), os.pardir, 'datasets', 'ma_dataset', 'combined', 'train')\n",
    "train_dataset = TusimpleSet(train_path, img_size=IMG_SIZE, transform=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=True)\n",
    "\n",
    "val_path = os.path.join(os.path.abspath(''), os.pardir, 'datasets', 'ma_dataset', 'combined', 'val')\n",
    "val_dataset = TusimpleSet(val_path, img_size=IMG_SIZE, transform=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=True)\n",
    "\n",
    "logger.info('created datasets and dataloader')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aced1656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init: Model\n",
    "import torch.optim as optim\n",
    "from SegCropNet.model.SegCropNet.SegCropNet import SegCropNet\n",
    "from SegCropNet.model.SegCropNet.average_meter import AverageMeter\n",
    "model = SegCropNet(arch='UNet').to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=0.0005)\n",
    "loss_type = 'CrossEntropyLoss'\n",
    "best_loss = float('inf')\n",
    "epoch_start = 0\n",
    "scheduler = None\n",
    "\n",
    "logger.info('created model and optimizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794474d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model if already existing\n",
    "checkpoint_path = os.path.join(os.path.abspath(''), 'insta_checkpoint.pt.tar')\n",
    "if os.path.isfile(checkpoint_path):\n",
    "    logger.info('found existing model')\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch_start = checkpoint['epoch']\n",
    "    best_loss = checkpoint['loss']\n",
    "    logger.info('loaded existing model for continuation of training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14180026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Cycle\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from SegCropNet.model.SegCropNet.loss import compute_loss\n",
    "\n",
    "for epoch in range(epoch_start, NUM_EPOCHS, 1):\n",
    "  model.train()\n",
    "  epoch_loss = AverageMeter()\n",
    "  epoch_loss_bin = AverageMeter()\n",
    "  epoch_loss_inst = AverageMeter()\n",
    "  train_iou = AverageMeter()\n",
    "  for train_batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{NUM_EPOCHS}'):\n",
    "    inputs = train_batch['input'].type(torch.FloatTensor).to(DEVICE)\n",
    "    binaries = train_batch['binary'].type(torch.LongTensor).to(DEVICE)\n",
    "    instances = train_batch['instance'].type(torch.FloatTensor).to(DEVICE)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    train_preds = model(inputs)\n",
    "    batch_loss = compute_loss(train_preds, binaries, instances, loss_type)\n",
    "\n",
    "    batch_loss[0].backward()\n",
    "    optimizer.step()\n",
    "    if scheduler != None:\n",
    "      scheduler.step()\n",
    "\n",
    "    epoch_loss.update(batch_loss[0].item(), inputs.size(0))\n",
    "    epoch_loss_bin.update(batch_loss[1].item(), inputs.size(0))\n",
    "    epoch_loss_inst.update(batch_loss[2].item(), inputs.size(0))\n",
    "    train_iou.update(batch_loss[4], inputs.size(0))\n",
    "\n",
    "  model.eval()\n",
    "  loss = 0.0\n",
    "  val_loss = AverageMeter()\n",
    "  val_loss_bin = AverageMeter()\n",
    "  val_loss_inst = AverageMeter()\n",
    "  val_iou = AverageMeter()\n",
    "  with torch.no_grad():\n",
    "    for val_batch in tqdm(val_loader, desc=f'Validation'):\n",
    "      inputs = val_batch[0]['input'].type(torch.FloatTensor).to(DEVICE)\n",
    "      binaries = val_batch[0]['binary'].type(torch.LongTensor).to(DEVICE)\n",
    "      instances = val_batch[0]['instance'].type(torch.FloatTensor).to(DEVICE)\n",
    "\n",
    "      val_preds = model(inputs)\n",
    "      loss = compute_loss(val_preds, binaries, instances, loss_type)\n",
    "\n",
    "      val_loss.update(loss[0].item(), inputs.size(0))\n",
    "      val_loss_bin.update(loss[1].item(), inputs.size(0))\n",
    "      val_loss_inst.update(loss[2].item(), inputs.size(0))\n",
    "      val_iou.update(loss[4], inputs.size(0))\n",
    "    print(f'train_loss: {epoch_loss.avg} | val_loss: {val_loss.avg}')\n",
    "\n",
    "    if val_loss.avg < best_loss:\n",
    "      best_loss = val_loss.avg\n",
    "      torch.save(model.state_dict(), 'best_insta.pt')\n",
    "      print('New Model saved')\n",
    "      logger.info(f'updated best_model in epoch {epoch+1} with \\n \\\n",
    "                  training_loss: total: {epoch_loss.avg} binary: {epoch_loss_bin.avg} instance: {epoch_loss_inst.avg} and \\\n",
    "                  validation_loss: total: {val_loss.avg} binary: {val_loss_bin.avg} instance: {val_loss_inst.avg}')\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "      torch.save({'epoch': epoch,\n",
    "                  'model_state_dict': model.state_dict(),\n",
    "                  'optimizer_state_dict': optimizer.state_dict(),\n",
    "                  'loss': best_loss},\n",
    "                  'insta_checkpoint.pt.tar')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
