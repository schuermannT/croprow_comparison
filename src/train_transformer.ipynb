{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67a5eb4-36a6-484e-92f3-63712b5bc8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a938195a-448a-44b5-be94-6d580db53938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(filename='transformer.log', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85cb681-5d05-45c4-bb15-60e3c27be0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters - according to paper\n",
    "NUM_EPOCHS = 1000\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 1e-4\n",
    "IMG_SIZE = (360, 640)\n",
    "NUM_QUERIES = 6\n",
    "NUM_WORKERS = 0#16\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d557ae8e-cecc-4dc2-a249-e2c915b63998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init: Datasets and Dataloader\n",
    "import os\n",
    "#from dataset import CropRowDataset\n",
    "from transformer_dataset import MaskLessDataset\n",
    "from torch.utils.data import DataLoader\n",
    "#train_path = os.path.join(os.path.abspath(''), 'dataset', 'train')\n",
    "train_path = os.path.join(os.path.abspath(''), os.pardir, 'datasets', 'ma_dataset', 'combined', 'train')\n",
    "train_dataset = MaskLessDataset(label_path = os.path.join(train_path, 'labels'), img_path = os.path.join(train_path, 'imgs'), param_size = NUM_QUERIES, img_size=IMG_SIZE)\n",
    "#train_dataset = CropRowDataset(label_path = os.path.join(train_path, 'masks'), img_path = os.path.join(train_path, 'imgs'), param_size=NUM_QUERIES)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers = NUM_WORKERS)\n",
    "#val_path = os.path.join(os.path.abspath(''), 'dataset', 'val')\n",
    "val_path = os.path.join(os.path.abspath(''), os.pardir, 'datasets', 'ma_dataset', 'combined', 'val')\n",
    "val_dataset = MaskLessDataset(label_path = os.path.join(val_path, 'labels'), img_path = os.path.join(val_path, 'imgs'), param_size = NUM_QUERIES, img_size=IMG_SIZE)\n",
    "#val_dataset = CropRowDataset(label_path = os.path.join(val_path, 'masks'), img_path = os.path.join(val_path, 'imgs'), param_size=NUM_QUERIES)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "logger.info('created datasets and dataloader')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10f8ff6-f661-477a-a2db-42fe58c5df87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init: Model\n",
    "import torch.optim as optim\n",
    "from transformer import TransformerBasedModel\n",
    "from hungarian_loss import HungarianLoss\n",
    "model = TransformerBasedModel(max_crop_rows=NUM_QUERIES).to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
    "loss_function = HungarianLoss()\n",
    "patience_counter = 0\n",
    "best_loss = float('inf')\n",
    "patience_limit = 20\n",
    "epoch_start = 0\n",
    "logger.info('created model and optimizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7eff8e0-7b2c-49cc-b4f7-6716c5673da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model if already existing\n",
    "checkpoint_path = os.path.join(os.path.abspath(''), 'transformer_checkpoint.pt.tar')\n",
    "if os.path.isfile(checkpoint_path):\n",
    "    logger.info('found existing model')\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch_start = checkpoint['epoch']\n",
    "    best_loss = checkpoint['loss']\n",
    "    patience_counter = checkpoint['patience_counter']\n",
    "    logger.info('loaded existing model for continuation of training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f38f86b-407e-4546-98c2-4e2a0fdd0459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Cycle\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "for epoch in range(epoch_start, NUM_EPOCHS, 1):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for train_batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{NUM_EPOCHS}'):\n",
    "        train_images = train_batch['image'].to(DEVICE)\n",
    "        train_gt_params = train_batch['gt'].to(DEVICE)\n",
    "        train_gt_classes = train_batch['class'].to(DEVICE)\n",
    "        \n",
    "        #===DEBUG===\n",
    "        # train_debug_img = train_batch['debug']\n",
    "        # plt.imshow(train_debug_img[0])\n",
    "        # plt.axis('off')\n",
    "        # plt.show()\n",
    "        #===DEBUG END===\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        train_pred_classes, train_pred_params = model(train_images)\n",
    "        batch_loss = loss_function(train_pred_params, train_pred_classes, train_gt_params, train_gt_classes, IMG_SIZE[0])\n",
    "\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += batch_loss.item()\n",
    "    avg_train_loss = epoch_loss / len(train_loader)\n",
    "    if epoch % 10 == 0:\n",
    "        torch.save({'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': best_loss,\n",
    "                    'patience_counter': patience_counter},\n",
    "                   'transformer_checkpoint.pt.tar')\n",
    "        logger.info(f'created checkpoint for epoch {epoch+1} with training_loss: {avg_train_loss}')\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for val_batch in tqdm(val_loader, desc='Validation'):\n",
    "            val_imgs = val_batch['image'].to(DEVICE)\n",
    "            val_gt_params = val_batch['gt'].to(DEVICE)\n",
    "            val_gt_classes = val_batch['class'].to(DEVICE)\n",
    "            val_pred_classes, val_pred_params = model(val_imgs)\n",
    "            val_loss += loss_function(val_pred_params, val_pred_classes, val_gt_params, val_gt_classes, IMG_SIZE[0])\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        print(f'train_loss: {avg_train_loss} | val_loss: {avg_val_loss}')\n",
    "        if avg_val_loss < best_loss:\n",
    "            best_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), 'best_transformer.pt')\n",
    "            print('New Model saved')\n",
    "            patience_counter = 0\n",
    "            logger.info(f'updated best_model in epoch {epoch+1} with val_loss: {avg_val_loss} and train_loss: {avg_train_loss}')\n",
    "        #else:\n",
    "            #patience_counter += 1\n",
    "            #if patience_counter >= patience_limit:\n",
    "                #print('Should: Early stop due to no improvement in validation')\n",
    "                #logger.info(f'Should: Early stopping in epoch {epoch}')\n",
    "                #break\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
